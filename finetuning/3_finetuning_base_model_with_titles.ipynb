{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5edc5fb1-7f0b-4e37-b189-e6902b4e1d2a",
   "metadata": {},
   "source": [
    "### training hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eece1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 98\n",
    "learning_rate = 5e-5\n",
    "momentum = (0.9,0.98)\n",
    "eps = 1e-6\n",
    "weight_decay = 0.2\n",
    "number_of_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6fe93-4ca2-45c3-b91a-aca4e59823f7",
   "metadata": {},
   "source": [
    "### load images and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0335ee92-9380-448a-8c6d-990e691bbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "image_path = train.product_images\n",
    "texts_file = train.product_titles\n",
    "\n",
    "texts_list = train.read_text(texts_file)\n",
    "images, titles = train.get_image_title(image_path, texts_list)\n",
    "\n",
    "print(len(images), \"|\", len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8ccd3-b2fe-40c3-b496-b08c0a07a989",
   "metadata": {},
   "source": [
    "### create and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef001bb9-4537-49c9-ba8c-4da13139269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train.image_title_dataset(images, titles)\n",
    "print(\"dataset size: \",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf54b0e-e64b-4d1e-8bc8-71255f92de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e46891-a648-4393-bec1-20bc9b6d441c",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = train.model\n",
    "preprocess = train.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = train.device\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdceb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=momentum,\n",
    "    eps=eps,\n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    progress = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in progress:\n",
    "        optimizer.zero_grad()\n",
    "        images, texts = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else :\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        progress.set_description(f\"Epoch {epoch+1}/{number_of_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = 'models/model_branch2.pt'\n",
    "torch.save(model, output_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
