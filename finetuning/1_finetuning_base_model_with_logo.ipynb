{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477365d9-de0b-45fc-ad70-4be6eee8284c",
   "metadata": {},
   "source": [
    "### training hyper-paramaeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 5e-5\n",
    "momentum = (0.9,0.98)\n",
    "eps = 1e-6\n",
    "weight_decay = 0.2\n",
    "number_of_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd412f-f2b0-41e9-8e9c-91b370540eaf",
   "metadata": {},
   "source": [
    "### load images and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d250de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train\n",
    "\n",
    "image_path = train.logo_images\n",
    "texts_file = train.logo_texts\n",
    "\n",
    "texts_list = train.read_text(texts_file)\n",
    "images, titles = train.get_image_title(image_path, texts_list)\n",
    "\n",
    "print(len(images), \"|\", len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437649b-5e26-4e32-9b31-53a67fd92e4c",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13f3e0-3e02-415c-a4df-62a0065c77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = train.image_title_dataset(images, titles)\n",
    "print(\"dataset size: \",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51616b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01634c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train.model\n",
    "preprocess = train.preprocess\n",
    "\n",
    "def convert_models_to_fp32(model):\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data.float()\n",
    "        p.grad.data = p.grad.data.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2444f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = train.device\n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4863024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=momentum,\n",
    "    eps=eps,\n",
    "    weight_decay=weight_decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1094a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    progress = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    for batch in progress:\n",
    "        optimizer.zero_grad()\n",
    "        images, texts = batch\n",
    "\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else :\n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "\n",
    "        progress.set_description(f\"Epoch {epoch+1}/{number_of_epochs}, Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e51c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'models/model_branch1_cp1.pt'\n",
    "torch.save(model, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
